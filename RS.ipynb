{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNv18b9cns3Q7F2k5C7ZQcT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrfo/RS_init/blob/main/RS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3JxaIZcWn5q"
      },
      "source": [
        "# RS 実装\n",
        "\n",
        "[満足化を通じた最適な自律的探索] https://www.jstage.jst.go.jp/article/pjsai/JSAI2018/0/JSAI2018_1Z304/_article/-char/ja/\n",
        "\n",
        "[論文要約] https://github.com/astrfo/AutonomousOptimalExplorationThroughSatisficing/blob/main/AutonomousOptimalExplorationThroughSatisficing.ipynb\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_xANSNHO5YD"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "%matplotlib inline"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guDY-K_7O_cO"
      },
      "source": [
        "class Environment(object):\n",
        "    \"\"\"\n",
        "    K本の各腕の確率を生成\n",
        "    当たりかハズレかを返す\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, K):\n",
        "        self._K = K\n",
        "        self.prob = np.array([np.random.rand() for _ in range(K)])\n",
        "        # print(f'prob: {self.prob}')\n",
        "        # print(f'OPT: {sum(self.prob) / K}')\n",
        "\n",
        "    def play(self, arm):\n",
        "        if self.prob[arm] > np.random.rand():\n",
        "            return 1\n",
        "        else:\n",
        "            return 0"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOKD5Dr-TLio"
      },
      "source": [
        "class Policy(object):\n",
        "    \"\"\"\n",
        "    各方策の実行\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, policy):\n",
        "        if 'RS-CH' == policy:\n",
        "            self.act_policy = self.RSFunc\n",
        "        if 'RS-OPT' == policy:\n",
        "            self.act_policy = self.RS_OPT\n",
        "        if 'e_greedy' == policy:\n",
        "            self._epsilon = 0.5\n",
        "            self.act_policy = self.e_greedy\n",
        "        if 'random' == policy:\n",
        "            self.act_policy = self.random_\n",
        "        if 'ThompsonSampling' == policy:\n",
        "            self.act_policy = self.ThompsonSampling\n",
        "\n",
        "\n",
        "    def RSFunc(self, _, RS):\n",
        "        return np.argmax(RS)\n",
        "\n",
        "    def RS_OPT(self, _, RS):\n",
        "        return np.argmax(RS)\n",
        "\n",
        "    def e_greedy(self, V, _):\n",
        "        if self._epsilon < np.random.rand():\n",
        "            return np.random.choice(np.where(V == V.max())[0])\n",
        "        else:\n",
        "            return np.random.randint(len(V))\n",
        "\n",
        "    def random_(self, V, _):\n",
        "        return np.random.randint(len(V))\n",
        "\n",
        "    # def ThompsonSampling(self, V, _):\n",
        "    #     return 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZifVrjoPCTa"
      },
      "source": [
        "class RSAgent(object):\n",
        "    \"\"\"\n",
        "    経験期待値の更新\n",
        "    RS価値関数の定義、更新\n",
        "    基準値alephの更新, KL情報量\n",
        "    RS-greedy選択肢を試行する\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, K, policy):\n",
        "        self._K = K\n",
        "        self._alpha = None\n",
        "        self._V = None\n",
        "        self._n = None\n",
        "        self._N = None\n",
        "        self.RS = None\n",
        "        self.aleph = None\n",
        "        self.aleph_opt = None\n",
        "        self._mu = None\n",
        "        self._policy = Policy(policy)\n",
        "\n",
        "    def initialize(self):\n",
        "        self._V = np.array([0.5] * self._K)\n",
        "        self._n = np.array([1e-8] * self._K)\n",
        "        self._N = 0\n",
        "        self.RS = np.zeros(self._K)\n",
        "        self.aleph = 1.0\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        self._alpha = 1 / (1 + self._n[arm])\n",
        "        self._V[arm] = (1 - self._alpha) * self._V[arm] + (reward * self._alpha)\n",
        "        self._n[arm] += 1\n",
        "        self._N += 1\n",
        "        self.RS = (self._n / self._N) * (self._V - self.aleph)\n",
        "\n",
        "    def update_aleph(self):\n",
        "        G = random.choice(np.where(self._V == self._V.max())[0])\n",
        "        self._mu = np.exp(-self._n * self.D_KL(self._V, self._V[G]))\n",
        "        aleph_list = self._V[G] * (1 - (self._V/self._V[G]) * self._mu) / (1 - self._mu)\n",
        "        np.nan_to_num(aleph_list, copy=False)\n",
        "        self.aleph = max(aleph_list)\n",
        "\n",
        "    def D_KL(self, p, q):\n",
        "        return p*np.log(p/q) + (1-p)*np.log( (1-p) / (1-q) )\n",
        "\n",
        "    def select_arm(self):\n",
        "        return self._policy.act_policy(self._V, self.RS)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIZhZYcUllNa"
      },
      "source": [
        "class Simulator(object):\n",
        "    \"\"\"\n",
        "    シミュレーションを行う\n",
        "    regretの計算\n",
        "    regret,stepsをplot\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, trial, step, K):\n",
        "        self._algorithm = [\"RS-CH\", \"RS-OPT\"]\n",
        "        self._env = None\n",
        "        self._prob = None\n",
        "        self._trial = trial\n",
        "        self._step = step\n",
        "        self._K = K\n",
        "        self._regret = np.zeros(step)\n",
        "        self._regretV = None\n",
        "        self.fig, self.ax = plt.subplots()\n",
        "\n",
        "    def run(self):\n",
        "        for algo in self._algorithm:\n",
        "            self._agent = RSAgent(self._K, algo)\n",
        "            print(f'algorithm: {algo}')\n",
        "            for t in range(self._trial):\n",
        "                self._env = Environment(self._K)\n",
        "                self._prob = self._env.prob\n",
        "                self._agent.initialize()\n",
        "                self._regretV = 0.0\n",
        "                for s in range(self._step):\n",
        "                    arm = self._agent.select_arm()\n",
        "                    reward = self._env.play(arm)\n",
        "                    self._agent.update(arm, reward)\n",
        "                    self.calc_regret(t, s, arm)\n",
        "                    if algo == \"RS-CH\": self._agent.update_aleph()\n",
        "                    if algo == \"RS-OPT\": self._agent.aleph = sum(self._prob) / self._K\n",
        "            self.print_regret()\n",
        "        self.fig.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def calc_regret(self, t, s, arm):\n",
        "        self._regretV += (self._prob.max() - self._prob[arm])\n",
        "        self._regret[s] += (self._regretV - self._regret[s]) / (t+1)\n",
        "\n",
        "    def print_regret(self):\n",
        "        self.ax.plot(np.arange(self._step), self._regret, linestyle='dashed')\n",
        "        self.ax.set_title(f'sim: {self._trial}, step: {self._step}, K: {self._K}')\n",
        "        self.ax.set_xlabel(\"steps\")\n",
        "        self.ax.set_ylabel(\"regret\")\n",
        "        self.ax.legend(labels=self._algorithm)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKE34olTSZ-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df03f6a-5aae-450e-ec1c-d4ac5abda6d6"
      },
      "source": [
        "def main():\n",
        "    trial = 10000\n",
        "    step = 1000000\n",
        "    K = 2\n",
        "    sim = Simulator(trial, step, K)\n",
        "    sim.run()\n",
        "\n",
        "start = time.time()\n",
        "main()\n",
        "elapsed_time = time.time() - start\n",
        "print(\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "algorithm: RS-CH\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        }
      ]
    }
  ]
}