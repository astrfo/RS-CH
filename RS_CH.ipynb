{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RS_CH.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxN1mE9V/PIjfLC+LG8lfW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrfo/RS-CH/blob/main/RS_CH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3JxaIZcWn5q"
      },
      "source": [
        "# RS-CH 実装\n",
        "\n",
        "[満足化を通じた最適な自律的探索] https://www.jstage.jst.go.jp/article/pjsai/JSAI2018/0/JSAI2018_1Z304/_article/-char/ja/\n",
        "\n",
        "[論文要約] https://github.com/astrfo/AutonomousOptimalExplorationThroughSatisficing/blob/main/AutonomousOptimalExplorationThroughSatisficing.ipynb\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_xANSNHO5YD"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guDY-K_7O_cO"
      },
      "source": [
        "class Environment(object):\n",
        "    \"\"\"\n",
        "    K本の各腕の確率を生成\n",
        "    当たりかハズレかを返す\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, K):\n",
        "        self._K = K\n",
        "        self.prob = np.array([np.random.rand() for _ in range(K)])\n",
        "\n",
        "    def play(self, arm):\n",
        "        if self.prob[arm] > np.random.rand():\n",
        "            return 1\n",
        "        else:\n",
        "            return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOKD5Dr-TLio"
      },
      "source": [
        "class Policy(object):\n",
        "    \"\"\"\n",
        "    各方策の実行\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, policy, param, K):\n",
        "        if 'RS-CH' == policy:\n",
        "            self.act_policy = self.RS_CH\n",
        "            self.aleph = param\n",
        "            self.RS = np.zeros(K)\n",
        "        if 'RS-OPT' == policy:\n",
        "            self.act_policy = self.RS_OPT\n",
        "            self.aleph = param\n",
        "            self.RS = np.zeros(K)\n",
        "        if 'e-greedy' == policy:\n",
        "            self.act_policy = self.e_greedy\n",
        "            self.epsilon = param\n",
        "        if 'random' == policy:\n",
        "            self.act_policy = self.random_\n",
        "        if 'ThompsonSampling' == policy:\n",
        "            self.act_policy = self.ThompsonSampling\n",
        "\n",
        "\n",
        "    def RS_CH(self, V, n, N):\n",
        "        G = random.choice(np.where(V == V.max())[0])        # greedyな腕\n",
        "        self.RS = (n / (N+1)) * (V - self.aleph)        # RS評価値の更新\n",
        "        RSG = (n[G] / (N+1)) * (V[G] - self.aleph)     # RS[G]の全ての j に対して\n",
        "        mu = np.exp(-n * self.D_KL(V, V[G]))        # μ^CHの計算 (24)\n",
        "        mu[G] = 0.0\n",
        "        self.aleph = V[G] * (1 - (V/V[G]) * mu) / (1 - mu)      # R^CHの計算 (25)\n",
        "        np.nan_to_num(self.aleph, copy=False, nan=0)\n",
        "        exceed_RSG_index = np.where(RSG <= self.RS)[0]     # RS[G]以上のindexを取り出す\n",
        "\n",
        "        if len(exceed_RSG_index) == 1:     # RS[G]が最も高い\n",
        "            arm = G\n",
        "        else:\n",
        "            if len(np.where(RSG < self.RS)[0]) == 1:     # RS[G] < RS[j]\n",
        "                arm = np.where(RSG < self.RS)[0][0]\n",
        "            else:\n",
        "                exceed_mu = mu[exceed_RSG_index]\n",
        "                max_mu_index = np.where(exceed_mu.max() == exceed_mu)[0]\n",
        "                if len(max_mu_index) == 1:      # 最も高いμが1つだった場合はその腕を選択\n",
        "                    arm = exceed_RSG_index[max_mu_index[0]]\n",
        "                else:                                                        # 最も高いμが２つ以上だった場合\n",
        "                    exceed_aleph = self.aleph[max_mu_index]\n",
        "                    max_aleph_index = np.where(exceed_aleph.max() == exceed_aleph)[0]\n",
        "                    if len(max_aleph_index) == 1:     # 最も高いalephが1つだった場合はその腕を選択\n",
        "                        arm = exceed_RSG_index[max_mu_index[max_aleph_index[0]]]\n",
        "                    else:                                                           # 最も高いalephが2つ以上だった場合はランダムに選択\n",
        "                        arm = np.random.choice(exceed_RSG_index[max_mu_index[max_aleph_index]])\n",
        "        return arm\n",
        "\n",
        "\n",
        "    def RS_OPT(self, V, n, N):\n",
        "        self.RS = (n / (N+1)) * (V - self.aleph)\n",
        "        return np.random.choice(np.where(self.RS == self.RS.max())[0])\n",
        "\n",
        "    def e_greedy(self, V, _, __):\n",
        "        if self.epsilon < np.random.rand():\n",
        "            return np.random.choice(np.where(V == V.max())[0])\n",
        "        else:\n",
        "            return np.random.randint(len(V))\n",
        "\n",
        "    def random_(self, V, _, __):\n",
        "        return np.random.randint(len(V))\n",
        "\n",
        "    def D_KL(self, p, q):\n",
        "        return p*np.log(p/q) + (1-p)*np.log( (1-p) / (1-q) )\n",
        "\n",
        "    # alpha, beta の初期値\n",
        "    # def ThompsonSampling(self, V, _, __):\n",
        "    #     states = [(0, 0) for _ in V]\n",
        "    #     action = lambda: np.argmax([np.random.beta(s[0] + 1, s[1] + 1) for s in states])\n",
        "    #     return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZifVrjoPCTa"
      },
      "source": [
        "class RSAgent(object):\n",
        "    \"\"\"\n",
        "    経験期待値の更新\n",
        "    RS価値関数の定義、更新\n",
        "    選択肢を選ぶ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, K, policy, param):\n",
        "        self._K = K\n",
        "        self._alpha = None\n",
        "        self._V = None\n",
        "        self._n = None\n",
        "        self._N = None\n",
        "        self.policy = Policy(policy, param, K)\n",
        "\n",
        "    def initialize(self):\n",
        "        self._V = np.array([0.5] * self._K)\n",
        "        self._n = np.array([1e-8] * self._K)\n",
        "        self._N = 0\n",
        "\n",
        "    def update(self, arm, reward):\n",
        "        self._alpha = 1 / (1 + self._n[arm])\n",
        "        self._V[arm] = (1 - self._alpha) * self._V[arm] + (reward * self._alpha)\n",
        "        self._n[arm] += 1\n",
        "        self._N += 1\n",
        "\n",
        "    def select_arm(self):\n",
        "        return self.policy.act_policy(self._V, self._n, self._N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIZhZYcUllNa"
      },
      "source": [
        "class Simulator(object):\n",
        "    \"\"\"\n",
        "    シミュレーションを行う\n",
        "    regretの計算\n",
        "    regret,stepsをplot\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, trial, step, K):\n",
        "        self._algorithm = {\"RS-CH\": np.array([1.0] * K)}\n",
        "        self._env = None\n",
        "        self._prob = None\n",
        "        self._trial = trial\n",
        "        self._step = step\n",
        "        self._K = K\n",
        "        self._regret = np.zeros(step)\n",
        "        self._regretV = None\n",
        "        self.fig, self.ax = plt.subplots()\n",
        "\n",
        "    def run(self):\n",
        "        for algo, param in self._algorithm.items():\n",
        "            self._agent = RSAgent(self._K, algo, param)\n",
        "            print(f'algorithm: {algo}, param: {param}')\n",
        "            for t in range(self._trial):\n",
        "                self._env = Environment(self._K)\n",
        "                self._prob = self._env.prob\n",
        "                if algo == 'RS-OPT': self._agent.policy.aleph = sum(sorted(self._prob, reverse=True)[:2]) / 2\n",
        "                self._agent.initialize()\n",
        "                self._regretV = 0.0\n",
        "                for s in range(self._step):\n",
        "                    arm = self._agent.select_arm()\n",
        "                    reward = self._env.play(arm)\n",
        "                    self._agent.update(arm, reward)\n",
        "                    self.calc_regret(t, s, arm)\n",
        "            self.print_regret()\n",
        "        self.fig.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def calc_regret(self, t, s, arm):\n",
        "        self._regretV += (self._prob.max() - self._prob[arm])\n",
        "        self._regret[s] += (self._regretV - self._regret[s]) / (t+1)\n",
        "\n",
        "    def print_regret(self):\n",
        "        self.ax.plot(np.arange(self._step), self._regret, linestyle='dashed')\n",
        "        self.ax.set_title(f'sim: {self._trial}, step: {self._step}, K: {self._K}')\n",
        "        self.ax.set_xlabel(\"steps\")\n",
        "        self.ax.set_ylabel(\"regret\")\n",
        "        self.ax.legend(labels=self._algorithm.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKE34olTSZ-n",
        "outputId": "f640f0ba-934d-4b33-ce67-279d39e50f87"
      },
      "source": [
        "def main():\n",
        "    trial = 50\n",
        "    step = 10**5\n",
        "    K = 8\n",
        "    sim = Simulator(trial, step, K)\n",
        "    sim.run()\n",
        "\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "main()\n",
        "elapsed_time = time.time() - start\n",
        "print(\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "algorithm: RS-CH, param: [1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        }
      ]
    }
  ]
}